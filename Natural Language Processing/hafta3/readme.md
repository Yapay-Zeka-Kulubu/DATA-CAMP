Harika bir rehber taslaÄŸÄ±. Ä°Ã§eriÄŸi Markdown formatÄ±nda, okunabilirliÄŸi artÄ±racak hiyerarÅŸik baÅŸlÄ±klar, listeler, koyu vurgular ve matematiksel notasyonlarla dÃ¼zenledim.AÅŸaÄŸÄ±da dÃ¼zenlenmiÅŸ versiyonu bulabilirsin:SÄ±fÄ±rdan LLM (BÃ¼yÃ¼k Dil Modeli) GeliÅŸtirme ve Kaynak RehberiBu rehber, Transformer mimarisini anlamak, sÄ±fÄ±rdan bir dil modeli eÄŸitmek ve bu sÃ¼recin arkasÄ±ndaki matematiÄŸi kavramak isteyenler iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r. AÅŸaÄŸÄ±daki bÃ¶lÃ¼mlerde, kod implementasyonundan teorik makalelere kadar ilgili konulara nasÄ±l Ã§alÄ±ÅŸabileceÄŸiniz detaylandÄ±rÄ±lmÄ±ÅŸtÄ±r.ğŸ›  Temel AraÃ§lar ve Kod DepolarÄ±Pratik uygulama ve gÃ¶rselleÅŸtirme iÃ§in ana kaynaklar ÅŸunlardÄ±r:1. SÄ±fÄ±rdan LLM EÄŸitimi (Kod)Link: github.com/malibayram/llm-from-scratchAÃ§Ä±klama: Bu depo, teorik bilginin koda dÃ¶kÃ¼lmÃ¼ÅŸ halidir. AÅŸaÄŸÄ±daki teorik baÅŸlÄ±klarÄ± Ã§alÄ±ÅŸÄ±rken, bu repodaki ilgili kod bloklarÄ±nÄ± inceleyerek matematiÄŸin Python/PyTorch karÅŸÄ±lÄ±ÄŸÄ±nÄ± gÃ¶rebilirsiniz.2. 3D LLM GÃ¶rselleÅŸtirmeLink: bbycroft.net/llmAÃ§Ä±klama: Bir LLM'in iÃ§indeki aÄŸÄ±rlÄ±klarÄ±n ve veri akÄ±ÅŸÄ±nÄ±n nasÄ±l gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ anlamak iÃ§in mÃ¼kemmel bir araÃ§tÄ±r. Ã–zellikle "Matrix Math" ve "Attention" bÃ¶lÃ¼mlerini Ã§alÄ±ÅŸÄ±rken bu siteyi aÃ§Ä±k tutmanÄ±z Ã¶nerilir.ğŸ“š Konu BazlÄ± Kaynak RehberiTransformer mimarisinin her bir parÃ§asÄ±nÄ± derinlemesine anlamak iÃ§in aÅŸaÄŸÄ±daki sÄ±rayÄ± ve ilgili kaynaklarÄ± takip edebilirsiniz.1. GiriÅŸ ve TarihÃ§eModern yapay zeka devrimi, sÄ±ralÄ± iÅŸlem yapan RNN ve LSTM gibi modellerin yerini, paralelleÅŸtirmeye olanak tanÄ±yan Transformer yapÄ±sÄ±nÄ±n almasÄ±yla baÅŸlamÄ±ÅŸtÄ±r.Okunacak Makale: "Attention Is All You Need" (Vaswani et al., 2017). Bu makale Transformer'Ä± tanÄ±tan ve devrimi baÅŸlatan orijinal Ã§alÄ±ÅŸmadÄ±r.Ä°ncelenecek Kaynak: DataCamp - "How Transformers Work". RNN'lerden Transformer'a geÃ§iÅŸin nedenlerini (paralelleÅŸtirme ve uzun mesafe baÄŸÄ±mlÄ±lÄ±klarÄ±) aÃ§Ä±klar.2. Tokenizasyon (Metni SayÄ±lara Ã‡evirme)Model metni doÄŸrudan anlayamaz, Ã¶nce onu parÃ§alara (token) ayÄ±rÄ±p sayÄ±lara dÃ¶nÃ¼ÅŸtÃ¼rmemiz gerekir.YÃ¶ntemler:BPE (Byte-Pair Encoding): GPT modellerinin kullandÄ±ÄŸÄ± yÃ¶ntemdir. SÄ±klÄ±k (frekans) tabanlÄ± bir birleÅŸtirme yapar.WordPiece (WPC): BERT modelinin kullandÄ±ÄŸÄ± yÃ¶ntemdir. OlasÄ±lÄ±k (likelihood) tabanlÄ± bir seÃ§im yaparak veriyi en iyi temsil eden parÃ§alarÄ± seÃ§er.Kaynak: "BPE Tokenizasyon AlgoritmasÄ±" (OÄŸuzhan Yenen) - BPE'nin Ã§alÄ±ÅŸma mantÄ±ÄŸÄ± ve kod Ã¶rneÄŸi iÃ§in.Kaynak: "WPC Tokenizasyon AlgoritmasÄ±" (OÄŸuzhan Yenen) - WordPiece ve BPE farkÄ±nÄ± anlamak iÃ§in.3. Embedding (GÃ¶mme) ve Uzayda AnlamKelimeler (tokenlar) yÃ¼ksek boyutlu vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r. Bu uzayda benzer anlama gelen kelimeler matematiksel olarak birbirine yakÄ±n durur.GÃ¶rselleÅŸtirme: 3Blue1Brown - "Transformers, the tech behind LLMs". "Kral - Erkek + KadÄ±n = KraliÃ§e" gibi vektÃ¶r aritmetiÄŸi Ã¶rneklerini gÃ¶rsel olarak anlatÄ±r.Matematik: StatQuest - "The matrix math behind transformer". Kelimelerin matris Ã§arpÄ±mÄ±yla nasÄ±l vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼ gÃ¶sterir.4. Positional Encoding (SÄ±ralama Bilgisi)Transformerlar veriyi paralel iÅŸlediÄŸi iÃ§in kelimelerin sÄ±rasÄ±nÄ± (cÃ¼mlenin neresinde olduÄŸunu) bilmezler. Bu bilgi yapay olarak eklenmelidir.Teori: "Attention Is All You Need" makalesi. SinÃ¼s ve KosinÃ¼s fonksiyonlarÄ±nÄ±n frekanslarÄ± kullanÄ±larak konum bilgisinin nasÄ±l eklendiÄŸi anlatÄ±lÄ±r.AÃ§Ä±klama: StatQuest videosunda bu deÄŸerlerin "Word Embedding" vektÃ¶rlerine toplama iÅŸlemiyle nasÄ±l eklendiÄŸi gÃ¶sterilir.5. Attention MekanizmasÄ± (Modelin Kalbi)Modelin hangi kelimenin diÄŸer hangi kelimelerle iliÅŸkili olduÄŸunu anladÄ±ÄŸÄ± kÄ±sÄ±mdÄ±r. Sorgu (Query), Anahtar (Key) ve DeÄŸer (Value) matrisleri burada devreye girer.Derinlemesine AnlatÄ±m: 3Blue1Brown - "Attention in transformers, step-by-step". Q, K ve V vektÃ¶rlerinin nasÄ±l oluÅŸtuÄŸunu ve "Masking" iÅŸleminin (gelecekteki kelimeleri gÃ¶rmeme) mantÄ±ÄŸÄ±nÄ± anlatÄ±r.Matematiksel Ä°ÅŸlem: StatQuest videosu, aÅŸaÄŸÄ±daki formÃ¼lÃ¼n adÄ±m adÄ±m matris Ã§arpÄ±mÄ± olarak nasÄ±l yapÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir:$$Attention(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$Kod KarÅŸÄ±lÄ±ÄŸÄ±: GitHub deposundaki self_attention veya benzeri isimli fonksiyonlarÄ± bu kaynaklarla eÅŸleÅŸtirerek inceleyin.6. Mimariler: Encoder vs. DecoderHer Transformer modeli aynÄ± deÄŸildir. Amaca gÃ¶re farklÄ± yapÄ±lar kullanÄ±lÄ±r.ShutterstockKeÅŸfetEncoder-Only (Ã–rn: BERT): Metni anlamak, sÄ±nÄ±flandÄ±rmak iÃ§in kullanÄ±lÄ±r.Decoder-Only (Ã–rn: GPT): Metin Ã¼retmek (bir sonraki kelimeyi tahmin etmek) iÃ§in kullanÄ±lÄ±r.Encoder-Decoder (Ã–rn: Ã‡eviri Modelleri): Orijinal "Attention Is All You Need" makalesindeki yapÄ±.Kaynak: Sebastian Raschka - "Understanding Encoder And Decoder LLMs". Hangi modelin hangi gÃ¶rev iÃ§in uygun olduÄŸunu aÃ§Ä±klar.7. EÄŸitim ve Softmax (OlasÄ±lÄ±k DaÄŸÄ±lÄ±mÄ±)Modelin Ã§Ä±ktÄ±sÄ±nÄ±n nasÄ±l anlamlÄ± bir metne dÃ¶nÃ¼ÅŸtÃ¼ÄŸÃ¼ ve eÄŸitimin nasÄ±l yapÄ±ldÄ±ÄŸÄ±.Softmax ve SÄ±caklÄ±k (Temperature): 3Blue1Brown videosunda, modelin Ã¼rettiÄŸi sayÄ±larÄ±n (logit) nasÄ±l 0 ile 1 arasÄ±nda olasÄ±lÄ±klara dÃ¶nÃ¼ÅŸtÃ¼ÄŸÃ¼ ve "Temperature" parametresinin yaratÄ±cÄ±lÄ±ÄŸÄ± nasÄ±l etkilediÄŸi anlatÄ±lÄ±r.Teacher Forcing: StatQuest videosunda, eÄŸitim sÄ±rasÄ±nda modelin bir Ã¶nceki kendi tahmini yerine gerÃ§ek doÄŸru cevabÄ± (target) girdi olarak alarak nasÄ±l daha hÄ±zlÄ± eÄŸitildiÄŸi anlatÄ±lÄ±r.ğŸ”— HÄ±zlÄ± EriÅŸim LinkleriOrijinal Makale (Paper): Attention Is All You Need (Vaswani et al.)GÃ¶rsel AnlatÄ±m (Video): 3Blue1Brown - Neural Networks PlaylistMatematiksel AnlatÄ±m (Video): StatQuest with Josh StarmerTÃ¼rkÃ§e Kaynak (Makale): OÄŸuzhan Yenen - Medium YazÄ±larÄ±
