# Transformer Architecture - GPT from Scratch

Bu proje, decoder-only transformer (GPT) mimarisini **tamamen sÄ±fÄ±rdan** PyTorch ile kodlar. BPE (Byte Pair Encoding) tokenizer dahil her ÅŸey scratch'ten implement edilmiÅŸtir. AtatÃ¼rk'Ã¼n Nutuk metni Ã¼zerinde eÄŸitilir.

## ğŸ¯ Ã–zellikler

âœ… **Tamamen SÄ±fÄ±rdan KodlanmÄ±ÅŸ:**
- BPE Tokenizer (Byte Pair Encoding)
- Multi-Head Self-Attention
- Positional Encoding
- Feed Forward Networks
- Transformer Blocks
- Complete GPT Model

âœ… **GPU DesteÄŸi:** CUDA ile hÄ±zlÄ± eÄŸitim
âœ… **HuggingFace Format:** Standart model kaydetme
âœ… **TÃ¼rkÃ§e Metin Ãœretimi:** Nutuk veri seti

## ğŸ“ Proje YapÄ±sÄ±

```
TransformerArt/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ embeddings.py      # Token ve pozisyon gÃ¶mmeleri
â”‚   â”œâ”€â”€ attention.py       # Multi-head self-attention
â”‚   â””â”€â”€ feedforward.py     # Feed-forward network
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ transformer_block.py  # Transformer bloÄŸu
â”‚   â””â”€â”€ gpt.py             # Tam GPT modeli
â”œâ”€â”€ bpe_tokenizer.py       # BPE tokenizer (sÄ±fÄ±rdan kodlanmÄ±ÅŸ!)
â”œâ”€â”€ data_loader.py         # Veri yÃ¼kleme ve BPE tokenization
â”œâ”€â”€ config.py              # Hiperparametreler
â”œâ”€â”€ train.py               # EÄŸitim scripti
â”œâ”€â”€ generate.py            # Metin Ã¼retimi
â”œâ”€â”€ model_utils.py         # Model kaydetme/yÃ¼kleme (HF format)
â”œâ”€â”€ nutuk.txt              # Veri seti
â””â”€â”€ requirements.txt       # BaÄŸÄ±mlÄ±lÄ±klar
```

## ğŸ—ï¸ Mimari

### BPE Tokenizer (Byte Pair Encoding)

Karakter seviyesi yerine **subword tokenization** kullanÄ±yoruz:

1. **BaÅŸlangÄ±Ã§:** Her karakter ayrÄ± bir token
2. **Ä°terasyon:** En sÄ±k gÃ¶rÃ¼len token Ã§iftini birleÅŸtir
3. **Tekrar:** Ä°stenen vocabulary boyutuna kadar devam et

**Avantajlar:**
- Daha kÃ¼Ã§Ã¼k vocabulary
- Bilinmeyen kelimeler iÃ§in daha iyi genelleme
- Daha verimli encoding

### Transformer Mimarisi

```
Input Tokens
  â†“
[BPE Tokenization]
  â†“
Token + Positional Embeddings
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Block 1 â”‚
â”‚  - Attention        â”‚
â”‚  - Add & Norm       â”‚
â”‚  - Feed Forward     â”‚
â”‚  - Add & Norm       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Transformer Block 2 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
... (4 blocks total)
  â†“
Layer Normalization
  â†“
Linear (vocab projection)
  â†“
Softmax â†’ Output Probabilities
```

### Hiperparametreler

```python
# Model Architecture
n_embd = 128        # Embedding dimension
n_head = 4          # Attention heads (128/4 = 32 per head)
n_layer = 4         # Transformer blocks
block_size = 128    # Context length
dropout = 0.1       # Dropout rate

# Training
batch_size = 32     # Batch size
max_iters = 1000    # Training iterations
learning_rate = 3e-4
vocab_size = 500    # BPE vocabulary size
```

## ğŸš€ Kurulum ve KullanÄ±m

### 1. Sanal Ortam OluÅŸtur

```bash
# Sanal ortam oluÅŸtur
python -m venv venv

# Aktif et (Windows)
.\venv\Scripts\activate

# Aktif et (Linux/Mac)
source venv/bin/activate
```

### 2. BaÄŸÄ±mlÄ±lÄ±klarÄ± YÃ¼kle

```bash
# PyTorch CUDA desteÄŸiyle (GPU iÃ§in)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124

# veya CPU iÃ§in
pip install torch numpy
```

### 3. Modeli EÄŸit

```bash
# EÄŸitimi baÅŸlat
python train.py
```

**EÄŸitim Ã‡Ä±ktÄ±sÄ±:**
```
ğŸš€ Loading data with BPE tokenization...
============================================================
Training BPE Tokenizer
============================================================
Initial vocabulary size: 107
Unique words: 89543
Merge 50/396: a n -> an
Merge 100/396: e r -> er
...
âœ… BPE training complete!
Final vocabulary size: 500
Number of merges: 393
============================================================

Using device: cuda
GPT Model initialized with 836,199 parameters

============================================================
Starting training...
============================================================
Step    0: train loss 6.2145, val loss 6.2138
Step  100: train loss 3.8421, val loss 3.8567
Step  200: train loss 3.2156, val loss 3.2289
...
Step  999: train loss 2.1234, val loss 2.1456

============================================================
Training completed!
============================================================

âœ… Model saved in HuggingFace format to: model_output/
   - pytorch_model.bin
   - config.json
   - tokenizer_config.json
   - README.md

============================================================
Generating sample text...
============================================================
[Ãœretilen metin burada gÃ¶rÃ¼necek]
```

### 4. Metin Ãœret

```bash
# Ä°nteraktif metin Ã¼retimi
python generate.py
```

**KullanÄ±m:**
```
Prompt: TÃ¼rkiye Cumhuriyeti
Max tokens (default 500): 300
Temperature (default 0.8): 0.7

Generating...
------------------------------------------------------------
[Model tarafÄ±ndan Ã¼retilen metin]
------------------------------------------------------------
```

## ğŸ“Š Model DetaylarÄ±

| Ã–zellik | DeÄŸer |
|---------|-------|
| **Model Tipi** | Decoder-only Transformer (GPT) |
| **Tokenization** | BPE (Byte Pair Encoding) |
| **Vocabulary Size** | 500 tokens |
| **Parameters** | ~836K |
| **Model Size** | 3.6 MB |
| **Context Length** | 128 tokens |
| **Training Data** | Nutuk (~1.6M characters) |
| **GPU Memory** | ~500 MB |

## ğŸ” BPE Tokenizer DetaylarÄ±

### NasÄ±l Ã‡alÄ±ÅŸÄ±r?

```python
from bpe_tokenizer import BPETokenizer

# Tokenizer oluÅŸtur ve eÄŸit
tokenizer = BPETokenizer(vocab_size=500)
tokenizer.train(text)

# Encode
text = "Merhaba dÃ¼nya"
tokens = tokenizer.encode(text)  # [45, 123, 67, 89, ...]

# Decode
decoded = tokenizer.decode(tokens)  # "merhaba dÃ¼nya"
```

### BPE AlgoritmasÄ±

1. **Initialization:** TÃ¼m karakterler vocabulary'de
2. **Iteration:**
   - En sÄ±k gÃ¶rÃ¼len token Ã§iftini bul
   - Bu Ã§ifti yeni bir token olarak birleÅŸtir
   - Vocabulary'ye ekle
3. **Repeat:** Hedef vocabulary boyutuna kadar

**Ã–rnek Merge Ä°ÅŸlemleri:**
```
Initial: ['m', 'e', 'r', 'h', 'a', 'b', 'a']
Merge 1: 'a' + 'b' -> 'ab'
Result:  ['m', 'e', 'r', 'h', 'ab', 'a']
Merge 2: 'e' + 'r' -> 'er'
Result:  ['m', 'er', 'h', 'ab', 'a']
...
```

## ğŸ“š Kod YapÄ±sÄ±

### BPE Tokenizer ([bpe_tokenizer.py](file:///c:/Users/w/Desktop/Kodlama/VsCode/HelloWorld/TransformerArt/bpe_tokenizer.py))

```python
class BPETokenizer:
    def train(text):        # BPE eÄŸitimi
    def encode(text):       # Text -> token IDs
    def decode(ids):        # Token IDs -> text
    def save(filepath):     # Tokenizer kaydet
    def load(filepath):     # Tokenizer yÃ¼kle
```

### Transformer Components

- **Embeddings:** Token + Positional embeddings
- **Attention:** Multi-head self-attention with causal masking
- **FeedForward:** 2-layer MLP with GELU
- **TransformerBlock:** Attention + FFN + Residual + LayerNorm
- **GPT:** Complete model assembly

## ğŸ“ Ã–ÄŸrenme KaynaklarÄ±

### Implemented Concepts

âœ… **BPE Tokenization:**
- Subword segmentation
- Vocabulary learning
- Merge operations

âœ… **Transformer Architecture:**
- Self-attention mechanism
- Multi-head attention
- Positional encoding
- Feed-forward networks
- Layer normalization
- Residual connections

âœ… **Training:**
- AdamW optimizer
- Learning rate scheduling
- Gradient descent
- Loss calculation

## ğŸ”§ Troubleshooting

### CUDA HatasÄ±
```bash
# CUDA versiyonunu kontrol et
nvidia-smi

# Uygun PyTorch versiyonunu kur
pip install torch --index-url https://download.pytorch.org/whl/cu124
```

### Memory HatasÄ±
```python
# config.py'de batch_size'Ä± kÃ¼Ã§Ã¼lt
batch_size = 16  # 32 yerine
```

## ğŸ“ Referanslar

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer
- [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909) - BPE Paper
- [Andrej Karpathy - nanoGPT](https://github.com/karpathy/nanoGPT)
- [noktali-virgul-ai-lectures](https://github.com/Cengineer00/noktali-virgul-ai-lectures)

## ğŸ“„ Lisans

MIT License - EÄŸitim amaÃ§lÄ± kullanÄ±m iÃ§in serbesttir.

