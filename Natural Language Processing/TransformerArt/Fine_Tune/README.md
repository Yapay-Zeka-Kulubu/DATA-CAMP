# Instruction Fine-Tuning

Bu klasÃ¶r, pre-trained GPT modelini instruction-following iÃ§in fine-tune etmeyi iÃ§erir.

## ğŸ“ Dosyalar

- `ft_config.py` - Fine-tuning konfigÃ¼rasyonu
- `dataset_loader.py` - HuggingFace dataset yÃ¼kleyici
- `fine_tune.py` - Fine-tuning scripti
- `inference.py` - Ä°nteraktif inference

## ğŸ¯ AmaÃ§

Pre-trained GPT modelini instruction-response formatÄ±nda fine-tune ederek, modelin kullanÄ±cÄ± talimatlarÄ±nÄ± takip etmesini saÄŸlamak.

## ğŸ“Š Dataset

**HuggingFace:** `CausalLM/GPT-4-Self-Instruct-Turkish`
- TÃ¼rkÃ§e instruction-response Ã§iftleri
- GPT-4 tarafÄ±ndan oluÅŸturulmuÅŸ
- 1000 sample kullanÄ±lÄ±yor (demo iÃ§in)

## ğŸš€ KullanÄ±m

### 1. BaÄŸÄ±mlÄ±lÄ±klarÄ± Kur

```bash
pip install datasets huggingface_hub
```

### 2. Fine-Tuning Yap

```bash
cd Fine_Tune
python fine_tune.py
```

**Beklenen Ã‡Ä±ktÄ±:**
```
============================================================
INSTRUCTION FINE-TUNING
============================================================

Loading Instruction Dataset
============================================================
Downloading from HuggingFace: CausalLM/GPT-4-Self-Instruct-Turkish
Loaded 1000 samples

Loading pre-trained BPE tokenizer...
Vocabulary size (with special tokens): 504

Formatting instruction-response pairs...
Processed 100/1000 samples
...

ğŸ“Š Dataset Statistics:
   Total tokens: 150,000
   Train tokens: 135,000
   Validation tokens: 15,000
============================================================

Loading pre-trained model...
Model parameters: 940,000

============================================================
Starting fine-tuning...
============================================================
Step    0: train loss 3.8421, val loss 3.8567
Step   50: train loss 2.9156, val loss 2.9289
Step  100: train loss 2.5234, val loss 2.5456
...
Step  499: train loss 1.8234, val loss 1.9456

============================================================
Fine-tuning completed!
============================================================

âœ… Fine-tuned model saved to: fine_tuned_model.pt
```

### 3. Ä°nteraktif Inference

```bash
python inference.py
```

**KullanÄ±m:**
```
ğŸ“ Instruction: TÃ¼rkiye'nin baÅŸkenti neresidir?
   Max tokens (default 200): 100
   Temperature (default 0.7): 0.7

ğŸ¤– Generating response...

------------------------------------------------------------
TÃ¼rkiye'nin baÅŸkenti Ankara'dÄ±r. 1923 yÄ±lÄ±nda Cumhuriyet'in 
ilanÄ±ndan sonra baÅŸkent olarak seÃ§ilmiÅŸtir.
------------------------------------------------------------
```

## âš™ï¸ KonfigÃ¼rasyon

```python
# ft_config.py
ft_batch_size = 8          # KÃ¼Ã§Ã¼k batch size
ft_block_size = 256        # Uzun context
ft_max_iters = 500         # Az iteration
ft_learning_rate = 1e-4    # DÃ¼ÅŸÃ¼k learning rate
max_samples = 1000         # 1000 sample
```

## ğŸ“ Format

**Instruction Format:**
```
<INST>instruction</INST><RESP>response</RESP>
```

**Ã–rnek:**
```
<INST>Python'da liste nasÄ±l oluÅŸturulur?</INST>
<RESP>Python'da liste kÃ¶ÅŸeli parantez kullanÄ±larak oluÅŸturulur: 
my_list = [1, 2, 3, 4, 5]</RESP>
```

## ğŸ“ Ã–ÄŸrenilen Kavramlar

- **Transfer Learning:** Pre-trained model kullanma
- **Fine-Tuning:** Specific task iÃ§in model adaptasyonu
- **Instruction Following:** Talimat takip etme
- **Special Tokens:** Format iÃ§in Ã¶zel tokenler
- **Lower Learning Rate:** Fine-tuning iÃ§in dÃ¼ÅŸÃ¼k LR

## ğŸ“Š Beklenen SonuÃ§lar

- Initial loss: ~3.8
- Final loss: ~1.8-2.0
- Model instruction formatÄ±nÄ± Ã¶ÄŸrenir
- TÃ¼rkÃ§e talimatlarÄ± takip eder
- TutarlÄ± yanÄ±tlar Ã¼retir
