# ğŸ“ SÄ±fÄ±rdan Fine-Tuning'e

**KapsamlÄ± Rehber: Teori + Kod + Matematik**

Bu dokÃ¼man, transformer mimarisini sÄ±fÄ±rdan anlamanÄ±z ve kendi modelinizi eÄŸitmeniz iÃ§in hazÄ±rlanmÄ±ÅŸtÄ±r. Her adÄ±mda hem teorik aÃ§Ä±klama hem kod Ã¶rneÄŸi bulacaksÄ±nÄ±z.

---

## ğŸ“š Ä°Ã§indekiler

1. [Transformer Mimarisi Genel BakÄ±ÅŸ](#1-transformer-mimarisi-genel-bakÄ±ÅŸ)
2. [BPE Tokenization](#2-bpe-tokenization)
3. [Embedding KatmanlarÄ±](#3-embedding-katmanlarÄ±)
4. [Multi-Head Self-Attention](#4-multi-head-self-attention)
5. [Feed-Forward Network](#5-feed-forward-network)
6. [Transformer Block](#6-transformer-block)
7. [Complete GPT Model](#7-complete-gpt-model)
8. [Pre-Training](#8-pre-training)
9. [Fine-Tuning](#9-fine-tuning)
10. [Ã–nemli Parametreler](#10-Ã¶nemli-parametreler)
11. [Debugging ve Ä°puÃ§larÄ±](#11-debugging-ve-iÌ‡puÃ§larÄ±)

---

## 1. Transformer Mimarisi Genel BakÄ±ÅŸ

### ğŸ¯ Ne YapÄ±yoruz?

Decoder-only transformer (GPT-style) modeli oluÅŸturuyoruz. Bu model:
- Metni token'lara ayÄ±rÄ±r (BPE)
- Her token'Ä± vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r (Embedding)
- Self-attention ile token'lar arasÄ± iliÅŸkileri Ã¶ÄŸrenir
- Bir sonraki token'Ä± tahmin eder (Language Modeling)

### ğŸ“Š Mimari Diyagram

![Transformer Architecture](file:///C:/Users/w/.gemini/antigravity/brain/5a4c8119-18b9-427e-97c7-36b7d3551dd1/uploaded_image_1765724093511.png)

**SaÄŸ Taraf (Decoder - Bizim Modelimiz):**
```
Input (Outputs shifted right)
    â†“
Output Embedding
    â†“
Positional Encoding
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Masked Multi-Head       â”‚  â† Gelecek token'larÄ± gÃ¶rmez
â”‚ Attention               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Add & Norm (Residual)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feed Forward            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Add & Norm (Residual)
    â†“
(NÃ— tekrar)
    â†“
Linear
    â†“
Softmax
    â†“
Output Probabilities
```

### ğŸ”‘ Temel Kavramlar

**1. Autoregressive (Otoregresif):**
- Model bir sonraki token'Ä± tahmin eder
- Sadece geÃ§miÅŸ token'lara bakar (causal masking)

**2. Decoder-Only:**
- Sadece decoder kÄ±smÄ±nÄ± kullanÄ±yoruz
- Encoder yok (BERT gibi modellerde var)

**3. Self-Attention:**
- Her token diÄŸer token'larla iliÅŸkisini Ã¶ÄŸrenir
- "TÃ¼rkiye'nin baÅŸkenti" â†’ "Ankara" iliÅŸkisi

---

## 2. BPE Tokenization

### ğŸ“– Teori

**Byte Pair Encoding (BPE)**, metni subword'lere ayÄ±rÄ±r.

**Neden Character-level deÄŸil?**
- âœ… Daha kÃ¼Ã§Ã¼k vocabulary
- âœ… Bilinmeyen kelimeler iÃ§in daha iyi
- âœ… Daha verimli

**Algoritma:**
```
1. BaÅŸlangÄ±Ã§: Her karakter ayrÄ± token
   "merhaba" â†’ ['m', 'e', 'r', 'h', 'a', 'b', 'a']

2. En sÄ±k gÃ¶rÃ¼len Ã§ifti birleÅŸtir
   'a' + 'b' Ã§ok sÄ±k â†’ 'ab'
   "merhaba" â†’ ['m', 'e', 'r', 'h', 'ab', 'a']

3. Tekrar et
   'e' + 'r' â†’ 'er'
   "merhaba" â†’ ['m', 'er', 'h', 'ab', 'a']

4. Hedef vocabulary boyutuna kadar devam
```

### ğŸ’» Kod Ä°ncelemesi

**Dosya:** [`bpe_tokenizer.py`](file:///c:/Users/w/Desktop/Kodlama/VsCode/HelloWorld/TransformerArt/bpe_tokenizer.py)

```python
class BPETokenizer:
    def train(self, text):
        # 1. Metni kelimelere ayÄ±r
        words = re.findall(r'\S+', text.lower())
        
        # 2. Her kelimeyi karakterlere ayÄ±r + </w> ekle
        vocab_words = {' '.join(list(word) + ['</w>']): freq 
                      for word, freq in word_freqs.items()}
        
        # 3. Ä°teratif merge
        for i in range(num_merges):
            # En sÄ±k Ã§ifti bul
            pairs = self.get_stats(vocab_words)
            best_pair = max(pairs, key=pairs.get)
            
            # Merge yap
            vocab_words = self.merge_vocab(best_pair, vocab_words)
            self.merges.append(best_pair)
```

**Ã–rnek Ã‡Ä±ktÄ±:**
```
Initial vocabulary: 76 characters
Merge 50/424:  a + n â†’ an
Merge 100/424: e + r â†’ er
Merge 200/424: l + a â†’ la
Final vocabulary: 500 tokens
```

### ğŸ¯ Ã–nemli Parametreler

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| `vocab_size` | 500 | Hedef vocabulary boyutu |
| `num_merges` | 424 | YapÄ±lacak merge sayÄ±sÄ± |

**Trade-off:**
- KÃ¼Ã§Ã¼k vocab â†’ Daha uzun sequence'ler
- BÃ¼yÃ¼k vocab â†’ Daha kÄ±sa sequence'ler ama daha fazla parametre

---

## 3. Embedding KatmanlarÄ±

### ğŸ“– Teori

**Token Embedding:**
- Her token'Ä± dense vector'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
- Ã–ÄŸrenilebilir (learnable)

**Positional Encoding:**
- Token'Ä±n dizideki pozisyonunu kodlar
- Transformer'da sÄ±ra bilgisi yok, bu yÃ¼zden gerekli

### ğŸ§® Matematik

**Token Embedding:**
```
E_token âˆˆ â„^(vocab_size Ã— n_embd)
token_id â†’ E_token[token_id] âˆˆ â„^n_embd
```

**Positional Embedding:**
```
E_pos âˆˆ â„^(block_size Ã— n_embd)
position â†’ E_pos[position] âˆˆ â„^n_embd
```

**Final Embedding:**
```
x = E_token[token_id] + E_pos[position]
```

### ğŸ’» Kod Ä°ncelemesi

**Dosya:** [`components/embeddings.py`](file:///c:/Users/w/Desktop/Kodlama/VsCode/HelloWorld/TransformerArt/components/embeddings.py)

```python
class Embeddings(nn.Module):
    def __init__(self, vocab_size, n_embd, block_size):
        # Token embedding tablosu
        self.token_embedding = nn.Embedding(vocab_size, n_embd)
        
        # Positional embedding tablosu (learnable)
        self.position_embedding = nn.Embedding(block_size, n_embd)
    
    def forward(self, idx):
        B, T = idx.shape  # Batch, Time
        
        # Token embeddings
        tok_emb = self.token_embedding(idx)  # (B, T, n_embd)
        
        # Position embeddings
        pos = torch.arange(T, device=idx.device)
        pos_emb = self.position_embedding(pos)  # (T, n_embd)
        
        # Topla
        x = tok_emb + pos_emb  # Broadcasting: (B,T,n_embd) + (T,n_embd)
        return x
```

**Ã–rnek:**
```python
# Input: [45, 123, 67, 89]  (4 token)
# vocab_size=500, n_embd=128

tok_emb = [[0.12, -0.34, ...],  # 128 dim
           [0.56, 0.78, ...],
           [-0.23, 0.45, ...],
           [0.89, -0.12, ...]]

pos_emb = [[0.01, 0.02, ...],   # Position 0
           [0.03, 0.04, ...],   # Position 1
           [0.05, 0.06, ...],   # Position 2
           [0.07, 0.08, ...]]   # Position 3

x = tok_emb + pos_emb  # Element-wise addition
```

### ğŸ¯ Ã–nemli Parametreler

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| `n_embd` | 128 | Embedding dimension |
| `vocab_size` | 500 | Vocabulary boyutu |
| `block_size` | 128 | Maximum sequence length |

**Parametre SayÄ±sÄ±:**
```
Token Embedding: vocab_size Ã— n_embd = 500 Ã— 128 = 64,000
Position Embedding: block_size Ã— n_embd = 128 Ã— 128 = 16,384
Toplam: 80,384 parametre
```

---

## 4. Multi-Head Self-Attention

### ğŸ“– Teori

**Self-Attention**, her token'Ä±n diÄŸer token'larla iliÅŸkisini Ã¶ÄŸrenir.

**Soru:** "TÃ¼rkiye'nin baÅŸkenti Ankara'dÄ±r" cÃ¼mlesinde "baÅŸkenti" kelimesi hangi kelimelere dikkat etmeli?
**Cevap:** "TÃ¼rkiye'nin" ve "Ankara'dÄ±r" â†’ Ä°liÅŸki Ã¶ÄŸrenilir!

### ğŸ§® Matematik

**Scaled Dot-Product Attention:**

```
Q = X Ã— W_Q    (Query)
K = X Ã— W_K    (Key)
V = X Ã— W_V    (Value)

Attention(Q, K, V) = softmax(Q Ã— K^T / âˆšd_k) Ã— V
```

**AdÄ±m adÄ±m:**

1. **Query, Key, Value hesapla:**
   ```
   Q, K, V âˆˆ â„^(T Ã— d_k)
   d_k = n_embd / n_head
   ```

2. **Attention scores:**
   ```
   scores = Q Ã— K^T âˆˆ â„^(T Ã— T)
   scores[i,j] = "token i, token j'ye ne kadar dikkat ediyor?"
   ```

3. **Scaling:**
   ```
   scores = scores / âˆšd_k
   ```
   Neden? BÃ¼yÃ¼k d_k'da gradient'ler Ã§ok kÃ¼Ã§Ã¼k olur.

4. **Causal Masking (Decoder iÃ§in):**
   ```
   mask[i,j] = 0 if j > i else 1
   scores = scores.masked_fill(mask == 0, -inf)
   ```
   Token i, sadece i'den Ã¶nceki token'lara bakabilir!

5. **Softmax:**
   ```
   attention_weights = softmax(scores)  # Her satÄ±r toplamÄ± 1
   ```

6. **Weighted sum:**
   ```
   output = attention_weights Ã— V
   ```

**Multi-Head:**
- Attention'Ä± paralel olarak n_head kez yap
- Her head farklÄ± iliÅŸkileri Ã¶ÄŸrenir
- SonuÃ§larÄ± concatenate et

### ğŸ’» Kod Ä°ncelemesi

**Dosya:** [`components/attention.py`](file:///c:/Users/w/Desktop/Kodlama/VsCode/HelloWorld/TransformerArt/components/attention.py)

```python
class MultiHeadAttention(nn.Module):
    def __init__(self, n_embd, n_head, block_size, dropout=0.1):
        self.n_head = n_head
        self.head_size = n_embd // n_head  # Her head'in boyutu
        
        # Q, K, V projections
        self.query = nn.Linear(n_embd, n_embd)
        self.key = nn.Linear(n_embd, n_embd)
        self.value = nn.Linear(n_embd, n_embd)
        
        # Output projection
        self.proj = nn.Linear(n_embd, n_embd)
        
        # Causal mask (lower triangular)
        self.register_buffer("mask",
            torch.tril(torch.ones(block_size, block_size)))
    
    def forward(self, x):
        B, T, C = x.shape  # Batch, Time, Channels
        
        # 1. Q, K, V hesapla
        q = self.query(x)  # (B, T, n_embd)
        k = self.key(x)
        v = self.value(x)
        
        # 2. Multi-head iÃ§in reshape
        # (B, T, n_embd) â†’ (B, T, n_head, head_size) â†’ (B, n_head, T, head_size)
        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)
        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)
        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)
        
        # 3. Attention scores
        att = (q @ k.transpose(-2, -1)) * (1.0 / (self.head_size ** 0.5))
        # (B, n_head, T, head_size) @ (B, n_head, head_size, T) 
        # = (B, n_head, T, T)
        
        # 4. Causal masking
        att = att.masked_fill(self.mask[:T, :T] == 0, float('-inf'))
        
        # 5. Softmax
        att = F.softmax(att, dim=-1)  # Her satÄ±r toplamÄ± 1
        
        # 6. Weighted sum
        y = att @ v  # (B, n_head, T, T) @ (B, n_head, T, head_size)
                     # = (B, n_head, T, head_size)
        
        # 7. Concatenate heads
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        
        # 8. Output projection
        y = self.proj(y)
        return y
```

**GÃ¶rsel Ã–rnek:**

```
Input: "TÃ¼rkiye'nin baÅŸkenti"
Tokens: [T1, T2, T3]

Attention Matrix (after softmax):
       T1    T2    T3
T1  [ 1.0   0     0  ]  â† T1 sadece kendine bakar
T2  [ 0.3  0.7   0  ]  â† T2, T1'e %30, kendine %70
T3  [ 0.2  0.5  0.3 ]  â† T3 hepsine bakabilir

Causal Mask (Ã¼st Ã¼Ã§gen -inf):
       T1    T2    T3
T1  [ OK   -âˆ   -âˆ  ]
T2  [ OK   OK   -âˆ  ]
T3  [ OK   OK   OK  ]
```

### ğŸ¯ Ã–nemli Parametreler

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| `n_head` | 4 | Attention head sayÄ±sÄ± |
| `head_size` | 32 | Her head boyutu (128/4) |
| `dropout` | 0.1 | Regularization |

**Parametre SayÄ±sÄ± (bir head iÃ§in):**
```
W_Q: n_embd Ã— n_embd = 128 Ã— 128 = 16,384
W_K: n_embd Ã— n_embd = 128 Ã— 128 = 16,384
W_V: n_embd Ã— n_embd = 128 Ã— 128 = 16,384
W_O: n_embd Ã— n_embd = 128 Ã— 128 = 16,384
Toplam: 65,536 parametre
```

---

## 5. Feed-Forward Network

### ğŸ“– Teori

**Feed-Forward Network (FFN)**, her pozisyona ayrÄ± ayrÄ± uygulanÄ±r.

**AmaÃ§:**
- Non-linearity eklemek
- Representation capacity artÄ±rmak

### ğŸ§® Matematik

```
FFN(x) = GELU(x Ã— W_1 + b_1) Ã— W_2 + b_2

W_1 âˆˆ â„^(n_embd Ã— 4*n_embd)  # Expansion
W_2 âˆˆ â„^(4*n_embd Ã— n_embd)  # Projection
```

**GELU (Gaussian Error Linear Unit):**
```
GELU(x) = x Ã— Î¦(x)
Î¦(x) = cumulative distribution function of standard normal
```

Neden GELU? ReLU'dan daha smooth, gradient flow daha iyi.

### ğŸ’» Kod Ä°ncelemesi

**Dosya:** [`components/feedforward.py`](file:///c:/Users/w/Desktop/Kodlama/VsCode/HelloWorld/TransformerArt/components/feedforward.py)

```python
class FeedForward(nn.Module):
    def __init__(self, n_embd, dropout=0.1):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd),  # Expand: 128 â†’ 512
            nn.GELU(),                       # Non-linearity
            nn.Linear(4 * n_embd, n_embd),  # Project: 512 â†’ 128
            nn.Dropout(dropout),
        )
    
    def forward(self, x):
        return self.net(x)  # (B, T, n_embd) â†’ (B, T, n_embd)
```

**Neden 4Ã— expansion?**
- Transformer paper'da standart
- Daha fazla capacity
- Trade-off: Parametre sayÄ±sÄ± vs performance

### ğŸ¯ Ã–nemli Parametreler

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| `expansion` | 4 | Ä°Ã§ katman boyutu Ã§arpanÄ± |
| `dropout` | 0.1 | Regularization |

**Parametre SayÄ±sÄ±:**
```
W_1: n_embd Ã— 4*n_embd = 128 Ã— 512 = 65,536
b_1: 4*n_embd = 512
W_2: 4*n_embd Ã— n_embd = 512 Ã— 128 = 65,536
b_2: n_embd = 128
Toplam: 131,712 parametre
```

---

## 6. Transformer Block

### ğŸ“– Teori

**Transformer Block**, attention ve FFN'i birleÅŸtirir.

**Ã–nemli:** Residual connections ve Layer Normalization!

### ğŸ§® Matematik

```
# Pre-LN (Pre-Layer Normalization) variant
x = x + Attention(LayerNorm(x))
x = x + FFN(LayerNorm(x))
```

**Neden Residual?**
- Gradient flow iyileÅŸir
- Derin network'ler eÄŸitilebilir
- Identity mapping Ã¶ÄŸrenilebilir

**Neden LayerNorm?**
- Training stabilizasyonu
- Batch'e baÄŸÄ±msÄ±z (BatchNorm'dan farklÄ±)

### ğŸ’» Kod Ä°ncelemesi

**Dosya:** [`model/transformer_block.py`](file:///c:/Users/w/Desktop/Kodlama/VsCode/HelloWorld/TransformerArt/model/transformer_block.py)

```python
class TransformerBlock(nn.Module):
    def __init__(self, n_embd, n_head, block_size, dropout=0.1):
        super().__init__()
        self.attention = MultiHeadAttention(n_embd, n_head, block_size, dropout)
        self.feed_forward = FeedForward(n_embd, dropout)
        self.ln1 = nn.LayerNorm(n_embd)  # Pre-attention
        self.ln2 = nn.LayerNorm(n_embd)  # Pre-FFN
    
    def forward(self, x):
        # Attention block + residual
        x = x + self.attention(self.ln1(x))
        
        # FFN block + residual
        x = x + self.feed_forward(self.ln2(x))
        
        return x
```

**GÃ¶rsel:**
```
Input x
  â†“
LayerNorm
  â†“
Attention â”€â”€â”
  â†“         â”‚
  + â†â”€â”€â”€â”€â”€â”€â”€â”˜  (Residual)
  â†“
LayerNorm
  â†“
FFN â”€â”€â”€â”€â”€â”€â”€â”
  â†“        â”‚
  + â†â”€â”€â”€â”€â”€â”€â”˜  (Residual)
  â†“
Output
```

### ğŸ¯ Parametre SayÄ±sÄ±

```
Attention: 65,536
FFN: 131,712
LayerNorm1: 2 Ã— n_embd = 256
LayerNorm2: 2 Ã— n_embd = 256
Toplam (1 block): 197,760 parametre
```

---

## 7. Complete GPT Model

### ğŸ“– Teori

**GPT Model**, tÃ¼m bileÅŸenleri birleÅŸtirir:
1. Embeddings
2. NÃ— Transformer Blocks
3. Final LayerNorm
4. Language Modeling Head

### ğŸ’» Kod Ä°ncelemesi

**Dosya:** [`model/gpt.py`](file:///c:/Users/w/Desktop/Kodlama/VsCode/HelloWorld/TransformerArt/model/gpt.py)

```python
class GPT(nn.Module):
    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout=0.1):
        super().__init__()
        
        # 1. Embeddings
        self.embeddings = Embeddings(vocab_size, n_embd, block_size)
        
        # 2. Transformer blocks (NÃ—)
        self.blocks = nn.ModuleList([
            TransformerBlock(n_embd, n_head, block_size, dropout)
            for _ in range(n_layer)
        ])
        
        # 3. Final LayerNorm
        self.ln_f = nn.LayerNorm(n_embd)
        
        # 4. Language Modeling Head
        self.lm_head = nn.Linear(n_embd, vocab_size)
    
    def forward(self, idx, targets=None):
        # Embeddings
        x = self.embeddings(idx)  # (B, T, n_embd)
        
        # Transformer blocks
        for block in self.blocks:
            x = block(x)
        
        # Final norm
        x = self.ln_f(x)
        
        # Logits
        logits = self.lm_head(x)  # (B, T, vocab_size)
        
        # Loss (if targets provided)
        if targets is not None:
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                targets.view(-1)
            )
            return logits, loss
        
        return logits, None
```

### ğŸ¯ Model Parametreleri

**KonfigÃ¼rasyon:**
```python
vocab_size = 500
n_embd = 128
n_head = 4
n_layer = 4
block_size = 128
```

**Toplam Parametre:**
```
Embeddings: 80,384
Transformer Blocks: 197,760 Ã— 4 = 791,040
Final LayerNorm: 256
LM Head: vocab_size Ã— n_embd = 500 Ã— 128 = 64,000
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Toplam: 935,680 parametre (~936K)
```

---

## 8. Pre-Training

### ğŸ“– Teori

**Pre-training**, modeli genel dil bilgisi Ã¶ÄŸretmek iÃ§in yapÄ±lÄ±r.

**AmaÃ§:**
- TÃ¼rkÃ§e dilbilgisi Ã¶ÄŸren
- Kelime iliÅŸkilerini Ã¶ÄŸren
- Genel representation Ã¶ÄŸren

**Dataset:** Nutuk (~1.6M karakter)

### ğŸ§® Loss Function

**Cross-Entropy Loss:**
```
L = -âˆ‘ y_true Ã— log(y_pred)

y_true: One-hot encoded gerÃ§ek token
y_pred: Model'in tahmin ettiÄŸi probability distribution
```

**Ã–rnek:**
```
GerÃ§ek token: "Ankara" (ID: 45)
Model tahminleri:
  Token 44: 0.1
  Token 45: 0.7  â† DoÄŸru
  Token 46: 0.2

Loss = -log(0.7) = 0.357
```

### ğŸ’» Training Loop

**Dosya:** [`train.py`](file:///c:/Users/w/Desktop/Kodlama/VsCode/HelloWorld/TransformerArt/train.py)

```python
def train():
    # 1. Model oluÅŸtur
    model = GPT(vocab_size, n_embd, n_head, n_layer, block_size, dropout)
    model = model.to(device)
    
    # 2. Optimizer
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
    
    # 3. Training loop
    for iter in range(max_iters):
        # Batch al
        xb, yb = data_loader.get_batch('train')
        
        # Forward pass
        logits, loss = model(xb, yb)
        
        # Backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Evaluate
        if iter % eval_interval == 0:
            losses = estimate_loss(model)
            print(f"Step {iter}: train {losses['train']:.4f}, val {losses['val']:.4f}")
```

### ğŸ¯ Hiperparametreler

| Parametre | DeÄŸer | AÃ§Ä±klama |
|-----------|-------|----------|
| `batch_size` | 32 | Paralel sequence sayÄ±sÄ± |
| `block_size` | 128 | Context length |
| `max_iters` | 1000 | Training iterations |
| `learning_rate` | 3e-4 | AdamW learning rate |
| `eval_interval` | 100 | Evaluation frequency |

**Learning Rate Schedule:**
- Constant 3e-4 (basit)
- Alternatif: Warmup + Cosine decay

### ğŸ“Š Training SÃ¼reci

```
Step    0: train loss 6.2443, val loss 6.2459  â† Random baÅŸlangÄ±Ã§
Step  100: train loss 4.9251, val loss 4.9413  â† Ã–ÄŸrenmeye baÅŸladÄ±
Step  200: train loss 4.3972, val loss 4.4524
Step  300: train loss 4.0376, val loss 4.1358
Step  400: train loss 3.8804, val loss 3.9958
Step  500: train loss 3.7768, val loss 3.9083
Step  600: train loss 3.7164, val loss 3.8530
Step  700: train loss 3.6581, val loss 3.8073
Step  800: train loss 3.6101, val loss 3.7897
Step  900: train loss 3.5336, val loss 3.7410
Step  999: train loss 3.4780, val loss 3.6632  â† Final
```

**Loss Reduction:** 6.24 â†’ 3.48 (**~44% improvement**)

### ğŸ” Ne Ã–ÄŸrendi?

Model ÅŸunlarÄ± Ã¶ÄŸrendi:
- âœ… TÃ¼rkÃ§e karakter dizilimleri
- âœ… Kelime yapÄ±larÄ±
- âœ… BazÄ± yaygÄ±n kelimeler
- âœ… Temel dilbilgisi kalÄ±plarÄ±

---

## 9. Fine-Tuning

### ğŸ“– Teori

**Fine-tuning**, pre-trained modeli specific task iÃ§in uyarlar.

**AmaÃ§:**
- Instruction-following Ã¶ÄŸren
- KullanÄ±cÄ± talimatlarÄ±nÄ± takip et
- TutarlÄ± yanÄ±tlar Ã¼ret

**Dataset:** GPT-4-Self-Instruct-Turkish (1000 samples)

### ğŸ§® Format

**Instruction Format:**
```
<INST>instruction</INST><RESP>response</RESP>
```

**Ã–rnek:**
```
<INST>TÃ¼rkiye'nin baÅŸkenti neresidir?</INST>
<RESP>TÃ¼rkiye'nin baÅŸkenti Ankara'dÄ±r.</RESP>
```

### ğŸ’» Fine-Tuning SÃ¼reci

**Dosya:** [`Fine_Tune/fine_tune.py`](file:///c:/Users/w/Desktop/Kodlama/VsCode/HelloWorld/TransformerArt/Fine_Tune/fine_tune.py)

```python
def fine_tune():
    # 1. Instruction dataset yÃ¼kle
    dataset = InstructionDataset(max_samples=1000)
    
    # 2. Pre-trained model yÃ¼kle
    model, config, _ = load_model_hf_format(pretrained_model_path, device)
    
    # 3. Vocabulary geniÅŸlet (special tokens iÃ§in)
    if dataset.vocab_size != config['vocab_size']:
        # Embedding layer'Ä± geniÅŸlet
        expand_embeddings(model, dataset.vocab_size)
    
    # 4. Optimizer (dÃ¼ÅŸÃ¼k learning rate!)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
    
    # 5. Fine-tuning loop
    for iter in range(ft_max_iters):
        xb, yb = dataset.get_batch('train')
        logits, loss = model(xb, yb)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### ğŸ¯ Fine-Tuning Hiperparametreleri

| Parametre | Pre-Training | Fine-Tuning | Neden? |
|-----------|--------------|-------------|--------|
| `learning_rate` | 3e-4 | 1e-4 | KÃ¼Ã§Ã¼k deÄŸiÅŸiklikler |
| `batch_size` | 32 | 8 | Daha az data |
| `max_iters` | 1000 | 500 | HÄ±zlÄ± converge |
| `block_size` | 128 | 256 | Uzun instructions |

**Neden dÃ¼ÅŸÃ¼k LR?**
- Pre-trained weights'i korumak
- Catastrophic forgetting'i Ã¶nlemek
- Stable fine-tuning

### ğŸ“Š Beklenen SonuÃ§lar

```
Step    0: train loss 3.8421, val loss 3.8567  â† Pre-trained'den baÅŸla
Step   50: train loss 2.9156, val loss 2.9289  â† Instruction format Ã¶ÄŸreniyor
Step  100: train loss 2.5234, val loss 2.5456
Step  200: train loss 2.1567, val loss 2.2134
Step  300: train loss 1.9876, val loss 2.0543
Step  400: train loss 1.8765, val loss 1.9876
Step  499: train loss 1.8234, val loss 1.9456  â† Final
```

---

## 10. Ã–nemli Parametreler

### ğŸ“Š Model Boyutu vs Performance

| n_embd | n_head | n_layer | Parameters | Training Time | Performance |
|--------|--------|---------|------------|---------------|-------------|
| 64 | 2 | 2 | ~200K | Ã‡ok hÄ±zlÄ± | DÃ¼ÅŸÃ¼k |
| 128 | 4 | 4 | ~936K | HÄ±zlÄ± | Orta |
| 256 | 8 | 6 | ~10M | Orta | Ä°yi |
| 512 | 16 | 12 | ~100M | YavaÅŸ | Ã‡ok iyi |

**Bizim Model:** 128/4/4 â†’ 936K params (Ã¶ÄŸrenme iÃ§in ideal)

### ğŸ¯ Parametre SeÃ§imi Rehberi

**1. Embedding Dimension (n_embd):**
- KÃ¼Ã§Ã¼k (64-128): HÄ±zlÄ±, basit tasks
- Orta (256-512): Genel kullanÄ±m
- BÃ¼yÃ¼k (768-1024): Complex tasks

**2. Attention Heads (n_head):**
- n_embd % n_head == 0 olmalÄ±!
- Daha fazla head â†’ Daha fazla farklÄ± iliÅŸki
- Tipik: n_embd / n_head = 32-64

**3. Number of Layers (n_layer):**
- Az (2-4): HÄ±zlÄ±, basit
- Orta (6-12): Standart
- Ã‡ok (24+): GPT-3 seviyesi

**4. Context Length (block_size):**
- KÄ±sa (128-256): HÄ±zlÄ±, az memory
- Uzun (512-2048): Daha fazla context
- Trade-off: Memory O(TÂ²)

**5. Batch Size:**
- KÃ¼Ã§Ã¼k (8-16): Az memory, noisy gradients
- BÃ¼yÃ¼k (32-64): Stable training
- GPU memory'e gÃ¶re ayarla

**6. Learning Rate:**
- Pre-training: 1e-4 to 3e-4
- Fine-tuning: 1e-5 to 1e-4
- Adam/AdamW iÃ§in tipik

---

## 11. Debugging ve Ä°puÃ§larÄ±

### ğŸ› YaygÄ±n Hatalar

**1. CUDA Out of Memory**
```
RuntimeError: CUDA out of memory
```

**Ã‡Ã¶zÃ¼m:**
```python
# config.py'de kÃ¼Ã§Ã¼lt
batch_size = 16  # 32 yerine
block_size = 64  # 128 yerine
n_embd = 64      # 128 yerine
```

**2. Loss NaN**
```
Step 100: train loss nan
```

**Nedenler:**
- Learning rate Ã§ok yÃ¼ksek
- Gradient explosion
- Numerical instability

**Ã‡Ã¶zÃ¼m:**
```python
# Learning rate kÃ¼Ã§Ã¼lt
learning_rate = 1e-4  # 3e-4 yerine

# Gradient clipping ekle
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**3. Token ID Out of Range**
```
RuntimeError: index out of range
```

**Ã‡Ã¶zÃ¼m:**
```python
# Token validation ekle
tokens = [min(t, vocab_size-1) for t in tokens]
```

**4. Loss DÃ¼ÅŸmÃ¼yor**

**Kontrol listesi:**
- âœ… Learning rate uygun mu?
- âœ… Data doÄŸru mu yÃ¼kleniyor?
- âœ… Model device'da mÄ±?
- âœ… Optimizer doÄŸru mu?

### ğŸ“ˆ Training Ä°zleme

**1. Loss Curves:**
```python
import matplotlib.pyplot as plt

plt.plot(train_losses, label='Train')
plt.plot(val_losses, label='Val')
plt.xlabel('Iteration')
plt.ylabel('Loss')
plt.legend()
plt.show()
```

**Ä°deal:**
- Train loss dÃ¼ÅŸÃ¼yor
- Val loss dÃ¼ÅŸÃ¼yor
- Gap kÃ¼Ã§Ã¼k (overfitting yok)

**2. Gradient Norms:**
```python
total_norm = 0
for p in model.parameters():
    param_norm = p.grad.data.norm(2)
    total_norm += param_norm.item() ** 2
total_norm = total_norm ** 0.5
print(f"Gradient norm: {total_norm}")
```

**Ä°deal:** 0.1 - 10 arasÄ±

**3. Text Generation:**
```python
# Her 100 iteration'da test et
if iter % 100 == 0:
    sample = model.generate(context, max_new_tokens=50)
    print(data_loader.decode(sample[0].tolist()))
```

### ğŸ’¡ Ä°puÃ§larÄ±

**1. BaÅŸlangÄ±Ã§:**
- âœ… KÃ¼Ã§Ã¼k model ile baÅŸla
- âœ… Az data ile test et
- âœ… Overfit edebiliyor mu kontrol et

**2. Scaling:**
- âœ… Ã–nce model boyutunu artÄ±r
- âœ… Sonra data'yÄ± artÄ±r
- âœ… Son olarak training time'Ä± artÄ±r

**3. Fine-Tuning:**
- âœ… DÃ¼ÅŸÃ¼k learning rate kullan
- âœ… Az iteration yeterli
- âœ… Validation loss'u izle

**4. GPU KullanÄ±mÄ±:**
```python
# GPU memory kullanÄ±mÄ±nÄ± izle
print(torch.cuda.memory_allocated() / 1024**2, "MB")
print(torch.cuda.memory_reserved() / 1024**2, "MB")
```

---

## ğŸ“ Ã–zet: AdÄ±m AdÄ±m EÄŸitim

### Pre-Training (SÄ±fÄ±rdan)

```bash
# 1. Sanal ortam
python -m venv venv
.\venv\Scripts\activate

# 2. BaÄŸÄ±mlÄ±lÄ±klar
pip install torch --index-url https://download.pytorch.org/whl/cu124

# 3. BPE Tokenizer eÄŸit + Model eÄŸit
python train.py

# Beklenen sÃ¼re: 2-3 dakika (CUDA)
# Beklenen loss: 6.24 â†’ 3.48
```

### Fine-Tuning

```bash
# 1. Fine-tuning klasÃ¶rÃ¼ne git
cd Fine_Tune

# 2. Dataset kÃ¼tÃ¼phaneleri
pip install datasets huggingface_hub

# 3. Fine-tuning
python fine_tune.py

# Beklenen sÃ¼re: 1-2 dakika
# Beklenen loss: 3.84 â†’ 1.82
```

### Inference

```bash
# Pre-trained model
python generate.py

# Fine-tuned model
cd Fine_Tune
python inference.py
```

---

## ğŸ“š Kaynaklar

**Papers:**
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Original Transformer
- [Neural Machine Translation with BPE](https://arxiv.org/abs/1508.07909) - BPE Algorithm
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) - GPT-3

**Code:**
- [Andrej Karpathy - nanoGPT](https://github.com/karpathy/nanoGPT)
- [HuggingFace Transformers](https://github.com/huggingface/transformers)

**Tutorials:**
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)

---

## ğŸ‰ SonuÃ§

Bu rehberde:
- âœ… BPE tokenization'Ä± sÄ±fÄ±rdan kodladÄ±k
- âœ… Self-attention mekanizmasÄ±nÄ± anladÄ±k
- âœ… Transformer bloklarÄ±nÄ± birleÅŸtirdik
- âœ… Pre-training yaptÄ±k
- âœ… Fine-tuning yaptÄ±k
- âœ… Her adÄ±mda matematik + kod gÃ¶rdÃ¼k

**Sonraki AdÄ±mlar:**
1. FarklÄ± hiperparametreler dene
2. Daha bÃ¼yÃ¼k dataset kullan
3. Model boyutunu artÄ±r
4. FarklÄ± tasks iÃ§in fine-tune et

**BaÅŸarÄ±lar!** ğŸš€
